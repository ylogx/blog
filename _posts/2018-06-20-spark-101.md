---
layout: post
title: Spark 101
subtitle: Spark basics
date: '2018-06-20T10:16:32.300348000+05:30'
author: Shubham Chaudhary
permalink: spark/101
comments: true
published: false
tags:
  - spark
  - scala
  - data
  - ml
---

## Setting up cluster
I have [previously posted][spark-setup] about setting up spark cluster in a jiffy using docker.


## Word Count

Word count problem is the Hello World of spark.
You are given a text file (here the shakespeare's macbeth as a txt file),
and we'll output every word and its frequency.
You can download this fill using following command:

```bash
wget http://www.textfiles.com/etext/AUTHORS/SHAKESPEARE/shakespeare-macbeth-46.txt
```

Now once this is done, and you've placed this in the working directory,
you can use the following code and put it in a scala file:

```scala
var tf = sc.textFile("shakespeare-macbeth-46.txt")

tf.flatMap(x => x.split("\\W+")  // split the text by whitespace - spaces, tabs, newlines
  .filter(x => x.matches("[A-Za-z]+") && x.length > 2))  // regex to find only words with length greater than 2
  .map(word => (word, 1))
  .groupBy(_._1)  // group by the word
  .mapValues(_.size)
  .sortBy(_._2, false)  // sort by the frequency
  .take(100)
```

Extracting into methods:

```scala
def clean_str(s: String) : Array[(String, Int)] = {
    val splits = s.split("\\W+")
    val filtered_words = splits.filter(x => x.matches("[A-Za-z]+") && x.length > 2)
    return filtered_words.map(word => (word, 1))
}

tf.map(clean_str)
  .flatMap(y=>y)
  .groupBy(_._1)
  .mapValues(_.size)
  .sortBy(_._2, false)
  .take(100)

// OR

tf.map(clean_str)
  .flatMap(y=>y)
  .reduceByKey(_+_)
  .sortBy(_._2, false)
  .take(100)
```



## Why Spark

Presto good for aggregated data.
But huge amount of data, huge processing, good in spark.


[spark-setup]: https://shubham.chaudhary.xyz/blog/docker/101#setting-up-a-cluster
